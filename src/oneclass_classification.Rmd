# One-Class Classification

*Regular classification* or *cost-sensitive classification* are supervised learning methods: *class labels are given* for each observation and are used to train the model. Classification methods, therefore, are provided with the information of which observations belongs to which class and thus can learn class patterns and apply those to new data. To learn the patterns for the classes a sufficient number of observations in each class is required and (nearly) balanced class sizes.

But what if there are no labels in the data?

And what if the class sizes are unbalanced?

And what if the classes are not just unbalanced, but one class has very a small number of observations?

If only the first question is true for your data, you can probably use *cluster* methods, if only the second question is true you can use  [special sample techniques](over_and_undersampling.md#imbalanced-classification-problems). But if all questions are true for your data, you probably want to apply *Anomaly Detection methods*.

An *anomaly* is "something that deviates from what is standard, normal, or expected"
[Oxford English Dictionary]. In the sense of data, anomalies are observations, that deviate from the majority of the data [Amer et al., 2013, p.1]. In different application domains anomalies are
also referred to as *outliers*, *discordant observations*, *exceptions*, *aberrations*, *surprises*, *peculiarities*, *contaminants* [Chandola et al., 2009, p.1], *novelties*, *noise* or *deviations* [Dau et al., 2014, p.2].

The goal of outlier/anomaly detection is to identify observations which di do not conform to an expected pattern [Chandola et al., 2009, p.1] [Hodge and Austin, 2004, p.1] [Dau et al., 2014, p.1]. 

A special case of *Anomaly Detection* is the *one-class classification*. As the name *one-class learning* already says, the problem is that most of the time we only observe measurements/data of one class, namely the *normal* class. Especially in real-world scenarios, it is easy to collect the normal situation, for example, production machines are probably nearly always run as it should, aircraft machinery works like it should and so on. It is hard to simulate the situation which is not normal, and even if we would be able to simulate this situation, we wouldn’t know if we cover all possible situations of anomalies, since anomalies can be of diverse nature.

## One-class classification

As described in the previous section *one-class classification* is a *semi-supervised* learning approach. A requirement to apply *one-class classification* methods is to have training data, that consist only consists of *normal* observations (therefore *semi-supervised*). The basic idea of one-class classification is to describe/find/learn the behaviour of normal data and use this description of normality for detecting further anomalies. An anomaly is thus predicted as the deviation from this learned behaviour. Description of normal behaviour is depended on the method used for anomaly detection. Several methods exist based on support vector machines (e.g. *one-class svm*), density (e.g. *lof* or *svm*), correlation, cluster analysis, neural networks (e.g. *autoencoder*) or ensemble techniques. 

Unfortunately, in real life settings, there is usually no way to test, if any or all anomalies were detected, as usually labels are not given. In [%mlr] a new class is introduced to handle anomalies and also to test the methods, in case benchmark data are used or labels do exist. This necessary to provide reproducibility of the experiments and comparability of the methods.

## Quick start
Here we show the [%mlr] workflow to train, make predictions, and evaluate a learner on a one-class classification problem. More details on each step are provided later.

```{r, results='hide', warning = FALSE}
devtools::install_github("mlr-org/mlr", ref = "oneclass_lof_dbscan")
library(mlr)
# get synthetic anomaly data, first 1000 observation are normal, the last 50 are anomalies
# this dataset has labels in the 'target' column for testing the model
data = getTaskData(oneclass2d.task)
train.set = 1:550
test.set = 551:1050

## 1) Define the task
## For one-class classification, unlike other learning problem, the label names of the 
## positive/anomaly and negative/normal class needs to be provided.
## Note, the target column is not used for training only for evaluating
task = makeOneClassTask(id = "oneclass", data = data, target = "Target", positive = "Anomaly", negative = "Normal")

## 2) Define the learner
## Currently four methods are implemented for one-class classification
## All learner can have the predicted type response or probability
## per default the predict.threshold is set to the 95%-quantile, but if the user have information about
## the ratio of anomalies in the data, he/she should set predict.type to "prob" and adapt the predict.threshold variable
lrn.ksvm = makeLearner("oneclass.ksvm", predict.type = "response")
lrn.svm = makeLearner("oneclass.svm", predict.type = "response")
lrn.lofactor = makeLearner("oneclass.lofactor", predict.type = "prob", k = 20, predict.threshold = 0.95)
lrn.lof = makeLearner("oneclass.lof", predict.type = "prob",  predict.threshold = 0.95)
## sometimes the h2o learner are not computational stable, therefore add learner settings,
## e.g. l1, max_w2 etc.
lrn.ae = makeLearner("oneclass.h2o.autoencoder", predict.type = "prob", predict.threshold = 0.95, 
  activation = "Tanh", reproducible = TRUE, l2 = 1e-5, sparse = TRUE, max_w2 = 10)

## 3) Fit the model
## The model is fitted ignoring that labels are provided (more information later)
mod.ksvm = train(lrn.ksvm, task, subset = train.set)  
mod.svm = train(lrn.svm, task, subset = train.set)  
mod.lofactor = train(lrn.lofactor, task, subset = train.set)  
mod.lof = train(lrn.lof, task, subset = train.set)  
mod.ae = train(lrn.ae, task, subset = train.set)  

## 4) Make predictions
## If predict.type = "prob", observation with high probability are more likely to be anomalous
pred.ksvm = predict(mod.ksvm, task, subset = test.set)
pred.svm = predict(mod.svm, task, subset = test.set)
pred.lofactor = predict(mod.lofactor, task, subset = test.set)
pred.lof = predict(mod.lof, task, subset = test.set)
pred.ae = predict(mod.ae, task, subset = test.set)

## 5) Evaluate the learner
## The provided labels are now used to evaluate the models above using measurement for binary classification
performance(pred.ksvm, measures = list(bac, f1, mmce))

## additional measurement for evaluating anomaly detection methods using labels are also implemented in R
## for using those measurement, create them first
rprecision = makePrecisionMeasure(id = "RPrecision", type = "rprecision", adjusted = FALSE)
precisionat5 = makePrecisionMeasure(id = "Precisionat5", p = 5, type = "precisionatp", adjusted = FALSE)
avgprecision = makePrecisionMeasure(id = "AvgPrecision", type = "avgprecision", adjusted = FALSE)
wac = makeWACMeasure(id = "wac", w = 0.6) #weight of the anomaly class is set to 0.6

performance(pred.svm, measures = list(rprecision, precisionat5, avgprecision, wac))
performance(pred.lofactor, measures = list(rprecision, precisionat5, avgprecision, wac))
performance(pred.lof, measures = list(rprecision, precisionat5, avgprecision, wac))

## additional measurement for evaluating anomaly detection methods without using labels are also implemented in R
## for using those measurement, create them first (here for demonstration purpose set low n.alpha and n.sim)
## for dimension <= 8 use makeAMVMeasure
amv = makeAMVMeasure(id = "AMV", alphas = c(0.8, 0.99), n.alpha = 10, n.sim = 100)
## for dimension > 8 use makeAMVhdMeasure
amvhd = makeAMVhdMeasure(id = "AMV", alphas = c(0.8, 0.99), n.alpha = 10, n.sim = 100)

performance(model = mod.ae, pred = pred.ae, task = task, measures = list(amv))
```

If one only has a few data sets without labels, it is recommended to plot the probability and set the threshold manually.
```{r, out.width = '75%'}
par(mfrow = c(1,2))
n = length(pred.lofactor$data$prob.Anomaly)
plot(1:n, pred.lofactor$data$prob.Anomaly, main = "lofactor", ylab = "probability for anomaly", xlab = "observation")
plot(1:n, pred.ae$data$prob.Anomaly, main = "autoencoder", ylab = "probability for anomaly", xlab = "observation")
```

## Create a task

Usually, anomalous data sets don't have labels and therefore are an unsupervised learning problem, however, the [%mlr] one-class classification learner inherits from the supervised class. This ensures that all binary classification measures can be used for evaluating the model’s performance in case labeled data is given.

The first step is to create a [task](&Task) to encapsulate the data set and further relevant information about a machine learning problem, for example, the name of the target variable for one-class classification.
But before creating a [OneClassTask](&Task) problem you have to bring your data in the right format. Since the one-class classification problem inherits from the supervised class, the data needs a class label column, if it doesn't have a column containing the class labels, create a synthetic column with only one class (e.g. name the column 'Target' and fill it with the label 'Normal'). This column is *not* used for training only for testing, to use binary performance measurement like for example *F1*, *auc*.
Create a [OneClassTask](&Task), if you have the right data format:
```{r}
data = getTaskData(oneclass2d.task)
train.set = 1:550
test.set = 551:1050

task = makeOneClassTask(id = "oneclass", data = data, target = "Target", positive = "Anomaly", negative = "Normal")
task
```
In contrast to other learning problem, you *always* need to specify which column is the label/target column and which label stands for the *anomaly* class (positive) and which for the *normal* class (negative). As already mentioned above one-class classification only trains on one-class, the normal class, so you need to tell [%mlr] what the other class is. 
In the following example, we used the synthetic created data from the already existing [oneclass2d.task](&oneclass2d.task) in [%mlr] for anomaly data, which has a label column.


## Constructing a learner
By constructing a [learner](learner.md#constructing-a-learner) you set the learning method you want to use for your machine learning problem. Moreover, you can set hyperparameters and prediction type etc.
One-class classification in [%mlr] currently provides three learning methods:
* One-Class Support Vector Machine (One-class SVM) [Schölkopf et al.2000]

* Autoencoder (Artificial neural network) [Dau et al. 2014]

* Local Outlier Factor (LOF) [Breunig et al. 2000]

All one-class classification learners in [%mlr] support response or probability output. The response output will state if the observation is predicted as *normal* or *anomaly*, more details on the response output will be specified when the learner is introduced below. For the probabilistic output in [%mlr], we are using *converted* anomaly scores, which are *not* probabilities. Anomaly scores are the usual output of anomaly detection methods indicating to what degree an observation is anomalous (relative to the other observations). The anomaly scores are constructed depending on the applied anomaly detection method. This can lead to different ranges while interpretation remains unchanged, therefore we normalize the anomaly score by using a converting method (For more information about the transformation see the help page of [&convertingScoresToProbability]). Although this output is called *probability* within the [%mlr] package, it should not exactly be interpreted as a probability but more as a normalized score.  

### One-class SVM
SVMs are well known for analysing data for classification and regression task. A special case of SVMs is the one-class SVMs, which is an unsupervised algorithm that learns a decision function for anomaly detection. The packages function 'svm()' in [%e1071] and 'ksvm' in [%kernlab] already implemented an algorithm to do so. [%mlr] is using those training function for the internal one-class svm calculation.

```{r}
lrn.svm = makeLearner("oneclass.svm", predict.type = "prob")
lrn.ksvm = makeLearner("oneclass.ksvm")
lrn.ksvm
```
The anomaly scores, also called decision values are the distance between the corresponding observation and the decision boundaries. Both packages also return a response output, which is received by applying a threshold to the decision values. The threshold is per default set to 0, or in other words scores are positive for normal points and negative for anomalies in [%e1071] and [%kernlab]. The lower the score the more likely the related observation is anomalous. When using the method in [%mlr], the output is automatically normalized to [0,1] by using a converting function defined in [&convertingScoresToProbability]. After the transformation $1-f(score)$ is used for the probability column.

### Autoencoder (Artificial neural network)
A special case of a neural network is an autoencoder, applicable to unsupervised learning. The main idea of an autoencoder is to set the output data identical to the input data in the neural network and to train the model to reconstruct the input, with the objective to minimize the reconstruction error [Dau et al.2014, p.314]. But this approach requires to only use the data with the normal class in the training, to make sure the network learn the characteristics of the normal class and therefore classifies previously unseen observations which match the normal pattern as normal otherwise as anomalies [Dau et al. 2014, p.311]. In **R** the package [%h2o] provides an algorithm for an autoencoder. Unlike [%h2o], in [%mlr] you can currently specify up to five layers, by first setting the number of layers in the *layer* parameter and for each layer the number of nodes with *nodes1*, *nodes2*, *nodes3*. Check [%h2o] for more information on the other parameter specification.

```{r}
lrn.h2o = makeLearner("oneclass.h2o.autoencoder", par.vals = list(reproducible = T, seed = 1234), 
  predict.type = "prob", predict.threshold = 0.95, reproducible = TRUE, 
  layers = 2, nodes1 = 20, nodes2 = 10,  
  # following settings were made to stabilise the model
  activation = "Tanh", l2 = 1e-5, sparse = TRUE, max_w2 = 10)
lrn.h2o
```

As explained above, the autoencoder is producing a reconstruction error (mse between output and input layer in [&h2o]), which is the anomaly score. A high score or high mse means that the reconstruction wasn’t accurate, and therefore the observation can’t be compressed by the autoencoder without losing information, which is an indicator that the observation is not following normal behavior. So high scores tend to imply that the observation was rather anomalous. According to [&convertingScoresToProbability], after transformation $f(score)$ is used for the probability column. If you set the *predict.type* as *response*, [&mlr] applies a soft threshold of the 95%-quantile to the scores. Scores above the 95%-quantile are considered to be an anomaly.

### Local Outlier Factor (LOF)

The *local outlier factor (LOF)* is the degree of being an outlier of a data point. The degree depends on the *local* density, that is the dense of the *k-nearest* neighbours. *LOF* basically compares the local density of one data point to the local density of its neighbours. A data point, which local density is comparable lower than its neighbours is more likely an anomaly. A low local density indicates that the observation lies in a region that is sparser than that of its neighbours, and thus it is more likely to be anomalous. [Breunig et.al 2000]. In **R** the packages, the 'lofactor' in [%DMwR] and 'lof' in [%dbscan] provides an algorithm for the LOF method. (Note: $k$ should be smaller than the number of observation in the data, otherweise 'lofactor' will return 'NAs' and 'lof' an error.)

```{r}
# The number of neighbours (k) that will be used in the calculation of the local 
# outlier factors must be set by the user
lrn.lofactor = makeLearner("oneclass.lofactor", k = 50, predict.type = "prob")
lrn.lof = makeLearner("oneclass.lof", k = 50, predict.type = "prob")
lrn.lof
```

According to [&convertingScoresToProbability], after transformation $f(score)$ is used for the probability column. If you set the *predict.type* as *response*, [&mlr] applies a soft threshold of the 95%-quantile to the scores. Scores above the 95%-quantile are considered to be an anomaly.

## Train
You can [&train] a model as usual with a oneclass learner and a oneclass task as input. You can pass a ``subset`` or a ``newdata set``.  
```{r, results='hide'}
mod.ksvm = train(lrn.ksvm, task, subset = train.set)  
mod.svm = train(lrn.svm, task, subset = train.set)
mod.lofactor = train(lrn.lofactor, task, subset = train.set)  
mod.lof = train(lrn.lof, task, subset = train.set)  
mod.ae = train(lrn.ae, task, subset = train.set) 
```

## Predict
Prediction can be done as usual in [%mlr] with [predict](&predict.WrappedModel) and by passing a trained model and either the task to the ``task`` argument or some new data to the ``newdata`` argument. As always you can specify a ``subset`` of the data which should be predicted. In section [Constructing a Learner)](oneclass_classification.md#constructing-a-learner), the details of the probability and response output are explained.
```{r, results='hide'}
pred.ksvm = predict(mod.ksvm, task, subset = test.set)
pred.svm = predict(mod.svm, task, subset = test.set)
pred.ae = predict(mod.ae, task, subset = test.set)
```

The training of LOF in [&mlr] is different than the other learners, in the way that the (&train) and the (&test) function are independent. Which means that the (&train) function returns a prediction of the train data and the (&test) functions return a prediction of the test data based on the test data themselves, without needing the trained model. [&mlr] returns a message to notify the user.
(To apply LOF from the [&DMwR] package and from the [&dbscan], no test and train data are needed. In order to keep the structure of mlr, to enable tuning and to allow binary supervised evaluation methods we modified the [&lofactor] function in [&mlr] so LOF in [&mlr] inherits from the supervised class although it is an unsupervised learning method. 
```{r}
pred.lofactor = predict(mod.lofactor, task, subset = test.set)
pred.lof = predict(mod.lof, task, subset = test.set)
```

## Performance
To assess the performance of the prediction, use the [&performance] function. As usually in [&mlr] you can specify by the `measures` argument which [measure(s)](measures.md) to calculate. The default measure for one-class classification is the *F1* (Note: if your data don't provide labels, switch to *AMV* or *AMVhd* explained below). Use [&listMeasures] to see all available measures. More details can be found on the [table of performance measures](measures.md) and the [&measures] documentation page and in this section.

If the true labels are given you can use binary classification evaluation methods.
```{R}
performance(pred.ksvm, measures = list(bac, f1, mmce))
```


Binary classification evaluation methods consider the two classes as equal. However, in anomaly detection, the anomaly class is more interesting for the user. [&mlr] also implemented additional evaluation methods for evaluating anomaly detection methods in case true labels are provided. One method is *weighted accuracy (wac)* a modification of the *balanced accuracy (bac)*, for which the user needs to set the weight (in [0,1]) of the importance of anomaly class relative to the normal class. If the weight is set to 0.5 the *wac* equals the *bac*. *Bac* measure returns values between 0 and 1, whereas 1 indicates perfect prediction.
```{R}
wac = makeWACMeasure(id = "wac", w = 0.6) #weight of the anomaly class is set to 0.6
performance(pred.svm, measures = list(wac))
```

Further measurements, specialising on evaluating anomaly detection models with given labels are *R-Precision measure/top p-accuracy*, *precision at p* and *average precision* (see [Campos et al., 2016]). For using this measures, you need to create them first with [&makePrecisionMeasure]. More information is in [Campos et al., 2016] and the help page [&makePrecisionMeasure]. All three measures return values between 0 and 1, whereas 1 indicates perfect prediction.
```{R}
## additional measurement for evaluating anomaly detection methods using labels are also implemented in R
## for using those measurement, create them first
rprecision = makePrecisionMeasure(id = "RPrecision", type = "rprecision", adjusted = FALSE)
precisionat5 = makePrecisionMeasure(id = "Precisionat5", p = 5, type = "precisionatp", adjusted = FALSE)
avgprecision = makePrecisionMeasure(id = "AvgPrecision", type = "avgprecision", adjusted = FALSE)

performance(pred.lof, measures = list(rprecision, precisionat5, avgprecision))
```
In case no labels are available, the previously introduces measures are not applicable. In that case [&mlr] provides the *Area under the Mass-Volume Curve (AMV)* measurement according to [Thomas et al., 2016] for dimension smaller than eight. This measure needs be created with [&makeAMVMeasure]. For datasets with greater dimension than eight [&mlr] provides the *Area under the Mass-Volume Curve for high dimensional data (AMVhd)* according to [Goix, 2016], analogous use [&makeAMVhdMeasure] to create the measurement. More information is provided in the named paper and the help page [&makeAMVMeasure] and [&makeAMVhdMeasure].
```{R}
## additional measurement for evaluating anomaly detection methods without using labels are also implemented in R
## for using those measurement, create them first (here for demonstration purpose set low n.alpha and n.sim)
## for dimension <= 8 use makeAMVMeasure
amv = makeAMVMeasure(id = "AMV", alphas = c(0.8, 0.99), n.alpha = 10, n.sim = 100)
## for dimension > 8 use makeAMVhdMeasure
amvhd = makeAMVhdMeasure(id = "AMV", alphas = c(0.8, 0.99), n.alpha = 10, n.sim = 100)
performance(model = mod.ae, pred = pred.ae, task = task, measures = list(amv))
```


## Tuning/benchmark experiment
A challenge in anomaly detection is to tune hyperparameters. If you have data with labels, you can tune a model using a supervised binary measurement and use this model on unlabelled data. If you don't have labeled data, you can tune a model using an unsupervised binary measurement like *amv* or *amvhd*. A tuning example for all implemented anomaly detection is provided below using the [oneclass2d.task](&oneclass2d.task).

### Defining the resampling strategy for one-class classification
For one-class classification tuning you can use the standard [resampling strategy] (resample.md#defining-the-resampling-strategy), especially if you don't have labels. But if you have labels it is recommended to use one of the resampling strategies designed for the one-class classification task, which are: ``OCHoldhout``, ``OCCV``, ``OCSubsample``, ``OCBootsrap``, ``OCRepCV``, to assure to only have normal data during the training. ``OCHoldhout``,``OCSubsample``, and ``OCBootsrap`` are basically sampling only normal observations for the training data set and the remaining observations, including all anomalies, are used for testing. ``OCCV`` and ``OCRepCV`` split the anomalous observations in k-folds, dropping the anomalies in the used fold for training. These strategies can be called as usual in [&mlr] with the [&makeResampleDesc] function.
```{r}
# Set outer resampling strategy
outer = makeResampleDesc("OCBootstrap", iters = 3)
rin = makeResampleInstance(outer, oneclass2d.task)

# Set inner resampling strategy
inner = makeResampleDesc("CV", iters = 3)
```

### Specifying the optimization algorithm
For tuning an anomaly detection learner, you need to always create a learner with a ``prob`` output. In case you want to tune an anomaly learner using

*1.  a supervised measurement (e.g. ``f1`` or ``average precision``), needs predicted labels, as those measures based on the number of correct predicted anomalies or true predicted normal points. These classes are set by applying a threshold to the probability output. The threshold can vary, therefore you should tune the threshold. 

*2. an unsupervised measurement like ``avm`` or ``amvhd``, no threshold-tuning is needed, but the algorithm of those measures are based on an anomaly scores, which is the probability output in [&mlr]
```{r}
# Set search strategy (maxit is low to run this example)
ctrl = makeTuneControlRandom(maxit = 5L, tune.threshold = TRUE)
```
 
### Performing the benchmark experiment

For conducting a benchmark experiment with three learners, you need to specify the search space for each learner. The procedure is described in [Tuning](tune.md) and [Benchmark](benchmark_experiments.md) section.  
```{r}
### parameter for svm
ps.svm = makeParamSet(
  makeNumericParam("nu",  lower = 0.01, upper = 0.99),
  makeDiscreteParam("kernel", values = c("linear", "polynomial", "radial", "sigmoid")),
  makeIntegerParam("degree", lower = 1L, upper = 4L, requires = quote(kernel == "polynomial")),
  makeNumericParam("coef0", lower = 0, upper = 3, requires = quote(kernel == "polynomial" || kernel == "sigmoid"))
)

### parameter for lofactor
ps.lofactor = makeParamSet(
  makeIntegerParam("k", lower = 20, upper = 200))

### parameter for lof
ps.lof = makeParamSet(
  makeIntegerParam("k", lower = 20, upper = 200))
```
For the *oneclass.ksvm* learner there is an exception for setting the search space for the ``sigma`` parameter, see [Tuning](tune.md#specifying-the-search-space).
```{r}
### parameter for ksvm
ps.ksvm = makeParamSet(
  makeNumericParam("nu",  lower = 0.01, upper = 0.99),
  makeDiscreteParam("kernel", values = c("vanilladot", "polydot", "rbfdot", "tanhdot")),
  makeIntegerParam("degree", lower = 1L, upper = 4L, requires = quote(kernel %in% c("polydot", "anovadot", "besseldot"))),
  # for sigma 
  makeNumericParam("sigma", lower = -10, upper = 10, trafo = function(x) 10^x, require = quote(kernel %in% c("rbfdot", "anovadot", "besseldot", "laplacedot"))),
  makeNumericParam("scale", lower = 0, upper = 1, requires = quote(kernel %in% c("polydot", "tanhdot"))),
  makeNumericParam("offset", lower = 0, upper = 3, requires = quote(kernel %in% c("polydot", "tanhdot"))),
  makeIntegerParam(id = "order", lower = 1, upper = 3, requires = quote(kernel == "besseldot"))
)
```
There is also an exception for the *oneclass.h2o.autoencoder*. [&mlr] enables to tune for number of layers and nodes up to three layers.
```{r}
### parameter for autoencoder
ps.h2o = makeParamSet(
  makeIntegerParam(id = "layers", lower = 1L, upper = 3L, default = 1L),
  makeIntegerParam(id = "nodes1", lower = 150L, upper = 250, default = 200L),
  makeIntegerParam(id = "nodes2", lower = 150L, upper = 250, default = 200L,
    requires = quote(layers > 1)),
  makeIntegerParam(id = "nodes3", lower = 150L, upper = 250, default = 200L,
    requires = quote(layers > 2))
)
```

After defining the search space for the learners, set the learners and apply the tuning wrapper like section [Benchmark Experiment](benchmark_experiments.md#tuning).
```{r}
# make lofactor learner and tunewrapper
lrn.svm = makeLearner("oneclass.svm", predict.type = "prob")
lrn.svm = makeTuneWrapper(lrn.svm, resampling = inner, par.set = ps.svm, measures = list(f1),
  control = ctrl, show.info = FALSE)

# make lofactor learner and tunewrapper
lrn.ksvm = makeLearner("oneclass.ksvm", predict.type = "prob")
lrn.ksvm = makeTuneWrapper(lrn.ksvm, resampling = inner, par.set = ps.ksvm, measures = list(f1),
  control = ctrl, show.info = FALSE)

# make lofactor learner and tunewrapper
lrn.lofactor = makeLearner("oneclass.lofactor", predict.type = "prob")
lrn.lofactor = makeTuneWrapper(lrn.lofactor, resampling = inner, par.set = ps.lofactor, measures = list(f1),
 control = ctrl, show.info = FALSE)

# make autoencoder learner and tunewrapper
lrn.h2o = makeLearner("oneclass.h2o.autoencoder", predict.type = "prob", activation = "Tanh", 
  reproducible = TRUE, mini_batch_size = 4, loss = "Quadratic", adaptive_rate = TRUE, l2=1e-5, 
  sparse = TRUE, max_w2=10, train_samples_per_iteration = -1)
lrn.h2o = makeTuneWrapper(lrn.h2o, resampling = inner, par.set = ps.h2o, measures = list(f1),
  control = ctrl, show.info = FALSE)

lrns = list(lrn.svm, lrn.ksvm, lrn.h2o, lrn.lofactor, lrn.lof)
res = benchmark(lrns, oneclass2d.task, rin, measures = list(f1))
bmr = getBMRPerformances(res, as.df = TRUE)
bmr
```

## References
Albert Thomas, Stephan Clemencon, Feuilard Vincent, and Alexandre Gramfort.
Learning hyperparameters for unsupervised anomaly detection. Anomaly
Detection Workshop, ICML 2016, 2016.

Alexandros Karatzoglou, Alex Smola, Kurt Hornik, and Achim Zeileis. kernlab – an
S4 package for kernel methods in R. Journal of Statistical Software, 11(9):1–20,
2004. URL http://www.jstatsoft.org/v11/i09/.

Anh Hoang Dau, Victor Ciesielski, and Andy Song. Anomaly detection using replicator
neural networks trained on examples of one class. In SEAL, pages 311–322,
2014.

Bernhard Schölkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C
Williamson. Estimating the support of a high-dimensional distribution. Neural
computation, 13(7):1443–1471, 2001.

Guilherme O Campos, Arthur Zimek, Jörg Sander, Ricardo JGB Campello, Barbora
Micenková, Erich Schubert, Ira Assent, and Michael E Houle. On the evaluation
of unsupervised outlier detection: measures, datasets, and an empirical study.
Data Mining and Knowledge Discovery, 30(4):891–927, 2016.

Nicolas Goix. How to evaluate the quality of unsupervised anomaly detection algorithms?
arXiv preprint arXiv:1607.01152, 2016.

Mennatallah Amer, Markus Goldstein, and Slim Abdennadher. Enhancing one-class
support vector machines for unsupervised anomaly detection. In Proceedings of
the ACM SIGKDD Workshop on Outlier Detection and Description, pages 8–15.
ACM, 2013.

Michael R Smith and Tony Martinez. Improving classification accuracy by identifying
and removing instances that should be misclassified. In Neural Networks
(IJCNN), The 2011 International Joint Conference on, pages 2690–2697. IEEE,
2011.

Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey.
ACM Comput. Surv., 41(3):15:1–15:58, July 2009. ISSN 0360-0300. doi: 10.
1145/1541880.1541882. URL http://doi.acm.org/10.1145/1541880.1541882.

Victoria Hodge and Jim Austin. A survey of outlier detection methodologies.
Artificial intelligence review, 22(2):85–126, 2004.

Vipin Kumar. Parallel and distributed computing for cybersecurity. IEEE
Distributed Systems Online, 6(10), 2005.


